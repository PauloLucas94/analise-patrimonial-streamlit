import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
from io import BytesIO
from pathlib import Path
import gc

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="AnÃ¡lise de PatrimÃ´nio",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

class PatrimonioAnalyzer:
    def __init__(self):
        self.data = None
        self.processed = False
    
    def load_data_from_folder(self, folder_path='data', use_chunks=True):
        """Carrega automaticamente todos os CSVs de uma pasta com otimizaÃ§Ã£o de memÃ³ria"""
        dataframes = []
        folder = Path(folder_path)
        
        if not folder.exists():
            st.warning(f"âš ï¸ Pasta '{folder_path}' nÃ£o encontrada. Criando pasta...")
            folder.mkdir(exist_ok=True)
            return False
        
        csv_files = list(folder.glob('*.csv'))
        
        if not csv_files:
            st.warning(f"âš ï¸ Nenhum arquivo CSV encontrado em '{folder_path}'")
            return False
        
        st.info(f"ğŸ“‚ Carregando {len(csv_files)} arquivo(s) CSV...")
        
        # Progress bar
        progress_bar = st.progress(0)
        
        # Colunas essenciais para reduzir uso de memÃ³ria
        essential_columns = [
            'LocalizaÃ§Ã£o', 'InventÃ¡rio', 'DenominaÃ§Ã£o do Imobilizado',
            'Vida', 'Valor AquisiÃ§Ã£o', 'Valor ContÃ¡bil',
            'Item ConsumÃ­vel Similar', 'Percentagem de Similaridade (%)',
            'Data IncorporaÃ§Ã£o', 'Justificativa de Reprova'
        ]
        
        for idx, csv_file in enumerate(csv_files):
            try:
                # Carrega apenas colunas essenciais se existirem
                df = pd.read_csv(
                    csv_file, 
                    encoding='utf-8',
                    usecols=lambda col: col.strip() in essential_columns,
                    low_memory=False
                )
                df['arquivo_origem'] = csv_file.name
                dataframes.append(df)
                
                progress_bar.progress((idx + 1) / len(csv_files))
                
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(
                        csv_file, 
                        encoding='latin-1',
                        usecols=lambda col: col.strip() in essential_columns,
                        low_memory=False
                    )
                    df['arquivo_origem'] = csv_file.name
                    dataframes.append(df)
                    progress_bar.progress((idx + 1) / len(csv_files))
                except Exception as e:
                    st.error(f"âœ— Erro ao carregar {csv_file.name}: {str(e)}")
            except Exception as e:
                st.error(f"âœ— Erro ao carregar {csv_file.name}: {str(e)}")
        
        progress_bar.empty()
        
        if dataframes:
            st.info("ğŸ”„ Consolidando dados...")
            self.data = pd.concat(dataframes, ignore_index=True)
            
            # Libera memÃ³ria
            del dataframes
            gc.collect()
            
            st.success(f"âœ“ {len(self.data):,} registros carregados!")
            return True
        return False
    
    def load_data_from_upload(self, uploaded_files):
        """Carrega dados dos arquivos CSV enviados via upload"""
        dataframes = []
        
        essential_columns = [
            'LocalizaÃ§Ã£o', 'InventÃ¡rio', 'DenominaÃ§Ã£o do Imobilizado',
            'Vida', 'Valor AquisiÃ§Ã£o', 'Valor ContÃ¡bil',
            'Item ConsumÃ­vel Similar', 'Percentagem de Similaridade (%)',
            'Data IncorporaÃ§Ã£o', 'Justificativa de Reprova'
        ]
        
        for uploaded_file in uploaded_files:
            try:
                df = pd.read_csv(
                    uploaded_file, 
                    encoding='utf-8',
                    usecols=lambda col: col.strip() in essential_columns,
                    low_memory=False
                )
                df['arquivo_origem'] = uploaded_file.name
                dataframes.append(df)
            except Exception as e:
                st.error(f"âœ— Erro ao carregar {uploaded_file.name}: {str(e)}")
        
        if dataframes:
            self.data = pd.concat(dataframes, ignore_index=True)
            del dataframes
            gc.collect()
            return True
        return False
    
    def load_data_from_url(self, urls):
        """Carrega dados de URLs (GitHub, Google Drive, etc)"""
        dataframes = []
        
        essential_columns = [
            'LocalizaÃ§Ã£o', 'InventÃ¡rio', 'DenominaÃ§Ã£o do Imobilizado',
            'Vida', 'Valor AquisiÃ§Ã£o', 'Valor ContÃ¡bil',
            'Item ConsumÃ­vel Similar', 'Percentagem de Similaridade (%)',
            'Data IncorporaÃ§Ã£o', 'Justificativa de Reprova'
        ]
        
        for url in urls:
            try:
                if 'drive.google.com' in url:
                    file_id = url.split('/d/')[1].split('/')[0]
                    url = f'https://drive.google.com/uc?id={file_id}'
                
                df = pd.read_csv(
                    url,
                    usecols=lambda col: col.strip() in essential_columns,
                    low_memory=False
                )
                df['arquivo_origem'] = f"URL_{len(dataframes)+1}"
                dataframes.append(df)
                st.success(f"âœ“ URL {len(dataframes)}: {len(df)} registros carregados")
            except Exception as e:
                st.error(f"âœ— Erro ao carregar URL: {str(e)}")
        
        if dataframes:
            self.data = pd.concat(dataframes, ignore_index=True)
            del dataframes
            gc.collect()
            return True
        return False
    
    def preprocess_data(self):
        """Preprocessa os dados com otimizaÃ§Ã£o de memÃ³ria"""
        if self.data is None:
            return False
        
        st.info("ğŸ”§ Processando dados...")
        
        self.data.columns = self.data.columns.str.strip()
        
        # ConversÃ£o de tipos para economizar memÃ³ria
        if 'Data IncorporaÃ§Ã£o' in self.data.columns:
            self.data['Data IncorporaÃ§Ã£o'] = pd.to_datetime(
                self.data['Data IncorporaÃ§Ã£o'], 
                errors='coerce',
                format='mixed'
            )
            self.data['Ano IncorporaÃ§Ã£o'] = self.data['Data IncorporaÃ§Ã£o'].dt.year.astype('Int16')
            self.data['Idade_Item'] = (2025 - self.data['Ano IncorporaÃ§Ã£o']).astype('Int16')
            
            # Remove coluna de data completa para economizar memÃ³ria
            self.data.drop('Data IncorporaÃ§Ã£o', axis=1, inplace=True)
        
        if 'Percentagem de Similaridade (%)' in self.data.columns:
            self.data['Similaridade_Num'] = pd.to_numeric(
                self.data['Percentagem de Similaridade (%)'].str.replace('%', ''), 
                errors='coerce'
            ).astype('float32')
        
        if 'Valor AquisiÃ§Ã£o' in self.data.columns:
            self.data['Valor_Aquisicao_Num'] = pd.to_numeric(
                self.data['Valor AquisiÃ§Ã£o'].str.replace('R$', '').str.replace('.', '').str.replace(',', '.').str.strip(),
                errors='coerce'
            ).astype('float32')
        
        if 'Valor ContÃ¡bil' in self.data.columns:
            self.data['Valor_Contabil_Num'] = pd.to_numeric(
                self.data['Valor ContÃ¡bil'].str.replace('R$', '').str.replace('.', '').str.replace(',', '.').str.strip(),
                errors='coerce'
            ).astype('float32')
        
        if 'Vida' in self.data.columns:
            self.data['Vida_Num'] = pd.to_numeric(self.data['Vida'], errors='coerce').astype('Int16')
        
        if 'Justificativa de Reprova' in self.data.columns:
            self.data['Decisao'] = self.data['Justificativa de Reprova'].apply(
                self._categorizar_decisao
            ).astype('category')
            
            self.data['Categoria_Similaridade'] = self.data['Similaridade_Num'].apply(
                self._categorizar_similaridade
            ).astype('category')
        
        if 'LocalizaÃ§Ã£o' in self.data.columns:
            self.data['Regiao'] = self.data['LocalizaÃ§Ã£o'].apply(
                self._extrair_regiao
            ).astype('category')
        
        # Converte colunas de texto para category para economizar memÃ³ria
        text_columns = ['arquivo_origem', 'InventÃ¡rio']
        for col in text_columns:
            if col in self.data.columns:
                self.data[col] = self.data[col].astype('category')
        
        gc.collect()
        self.processed = True
        st.success("âœ“ Processamento concluÃ­do!")
        return True
    
    def _categorizar_decisao(self, justificativa):
        if pd.isna(justificativa):
            return "Sem Categoria"
        
        justificativa = str(justificativa).upper()
        
        if "RECLASSIFICAR" in justificativa:
            return "RECLASSIFICAR"
        elif "AVALIAR" in justificativa:
            return "AVALIAR"
        elif "MANTER" in justificativa:
            return "MANTER"
        else:
            return "OUTROS"
    
    def _categorizar_similaridade(self, similaridade):
        if pd.isna(similaridade):
            return "Sem Dados"
        
        if similaridade >= 70:
            return "Muito Alta (â‰¥70%)"
        elif similaridade >= 50:
            return "Alta (50-69%)"
        elif similaridade >= 35:
            return "Moderada (35-49%)"
        else:
            return "Baixa (<35%)"
    
    def _extrair_regiao(self, localizacao):
        if pd.isna(localizacao):
            return "NÃ£o Informado"
        
        return str(localizacao).strip()
    
    @st.cache_data(ttl=3600)
    def get_filtered_data(_self, regiao_filter=None, decisao_filter=None, 
                         ano_min=None, ano_max=None, similaridade_min=None):
        """Retorna dados filtrados com cache"""
        if not _self.processed:
            return pd.DataFrame()
        
        filtered = _self.data.copy()
        
        if regiao_filter and regiao_filter != "Todas":
            filtered = filtered[filtered['Regiao'] == regiao_filter]
        
        if decisao_filter and decisao_filter != "Todas":
            filtered = filtered[filtered['Decisao'] == decisao_filter]
        
        if ano_min is not None:
            filtered = filtered[filtered['Ano IncorporaÃ§Ã£o'] >= ano_min]
        
        if ano_max is not None:
            filtered = filtered[filtered['Ano IncorporaÃ§Ã£o'] <= ano_max]
        
        if similaridade_min is not None:
            filtered = filtered[filtered['Similaridade_Num'] >= similaridade_min]
        
        return filtered

@st.cache_data
def create_decision_pie_chart(data):
    """GrÃ¡fico de pizza das decisÃµes"""
    decisoes = data['Decisao'].value_counts()
    
    fig = px.pie(
        values=decisoes.values, 
        names=decisoes.index,
        title="DistribuiÃ§Ã£o de DecisÃµes",
        color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    )
    
    fig.update_traces(textposition='inside', textinfo='percent+label')
    return fig

@st.cache_data
def create_similarity_histogram(data):
    """Histograma de similaridade"""
    fig = px.histogram(
        data, 
        x='Similaridade_Num',
        nbins=20,
        title="DistribuiÃ§Ã£o de Similaridade (%)",
        labels={'Similaridade_Num': 'Similaridade (%)', 'count': 'Quantidade'}
    )
    
    fig.update_layout(showlegend=False)
    return fig

@st.cache_data
def create_regional_bar_chart(data):
    """GrÃ¡fico de barras por localizaÃ§Ã£o"""
    localizacao_counts = data['Regiao'].value_counts()
    
    fig = px.bar(
        x=localizacao_counts.index,
        y=localizacao_counts.values,
        title="Itens por LocalizaÃ§Ã£o",
        labels={'x': 'LocalizaÃ§Ã£o', 'y': 'Quantidade'}
    )
    
    fig.update_layout(
        showlegend=False,
        xaxis=dict(tickangle=45)
    )
    return fig

@st.cache_data
def create_similarity_box_plot(data):
    """Box plot de similaridade por decisÃ£o"""
    fig = px.box(
        data,
        x='Decisao',
        y='Similaridade_Num',
        title="Similaridade por Tipo de DecisÃ£o",
        labels={'Similaridade_Num': 'Similaridade (%)', 'Decisao': 'DecisÃ£o'}
    )
    
    return fig

@st.cache_data
def create_timeline_chart(data):
    """GrÃ¡fico temporal"""
    timeline = data.groupby(['Ano IncorporaÃ§Ã£o', 'Decisao']).size().unstack(fill_value=0)
    
    fig = go.Figure()
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    for i, decisao in enumerate(timeline.columns):
        fig.add_trace(
            go.Scatter(
                x=timeline.index,
                y=timeline[decisao],
                mode='lines+markers',
                name=decisao,
                line=dict(color=colors[i % len(colors)])
            )
        )
    
    fig.update_layout(
        title="Timeline de IncorporaÃ§Ãµes por DecisÃ£o",
        xaxis_title="Ano",
        yaxis_title="Quantidade",
        hovermode='x unified'
    )
    
    return fig

def create_strategic_metrics(data):
    """MÃ©tricas estratÃ©gicas"""
    total_itens = len(data)
    
    if total_itens == 0:
        return {}, {}
    
    decisoes = data['Decisao'].value_counts()
    reclassificar = decisoes.get('RECLASSIFICAR', 0)
    avaliar = decisoes.get('AVALIAR', 0)
    manter = decisoes.get('MANTER', 0)
    
    similaridade_stats = data['Similaridade_Num'].describe()
    idade_media = data['Idade_Item'].mean() if 'Idade_Item' in data.columns else 0
    
    metrics = {
        'total_itens': total_itens,
        'reclassificar': reclassificar,
        'avaliar': avaliar,
        'manter': manter,
        'percentual_reclassificar': (reclassificar / total_itens) * 100,
        'percentual_avaliar': (avaliar / total_itens) * 100,
        'similaridade_media': similaridade_stats.get('mean', 0),
        'alta_similaridade': len(data[data['Similaridade_Num'] >= 70]),
        'idade_media': idade_media
    }
    
    regional_analysis = data.groupby('Regiao').agg({
        'Decisao': lambda x: (x == 'RECLASSIFICAR').sum(),
        'Similaridade_Num': 'mean'
    }).round(2)
    
    regional_analysis.columns = ['Itens_Reclassificar', 'Similaridade_Media']
    
    return metrics, regional_analysis

def export_to_excel(data):
    """Exporta dados para Excel"""
    output = BytesIO()
    
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        # Limita exportaÃ§Ã£o para evitar problemas de memÃ³ria
        sample_size = min(len(data), 100000)
        if len(data) > sample_size:
            st.warning(f"âš ï¸ Exportando apenas {sample_size:,} registros de {len(data):,} para evitar problemas de memÃ³ria")
            data_to_export = data.sample(n=sample_size)
        else:
            data_to_export = data
        
        data_to_export.to_excel(writer, sheet_name='Dados_Completos', index=False)
        
        decisao_summary = data.groupby('Decisao').agg({
            'Similaridade_Num': ['count', 'mean', 'min', 'max'],
            'Ano IncorporaÃ§Ã£o': ['min', 'max']
        }).round(2)
        decisao_summary.to_excel(writer, sheet_name='Resumo_Decisoes')
        
        regional_summary = data.groupby('Regiao').agg({
            'Decisao': lambda x: x.value_counts().to_dict(),
            'Similaridade_Num': ['count', 'mean']
        })
        regional_summary.to_excel(writer, sheet_name='Resumo_Regional')
    
    return output.getvalue()

def create_demo_data():
    """Cria dados de demonstraÃ§Ã£o"""
    np.random.seed(42)
    n_samples = 1000
    
    sample_data = {
        'LocalizaÃ§Ã£o': np.random.choice(['1.01 BRÃS', '1.02 VILA ALPINA', '2.01 SANTOS', 'SENAI SEDE', '3.01 TAUBATÃ‰'], n_samples),
        'InventÃ¡rio': range(1000000, 1000000 + n_samples),
        'DenominaÃ§Ã£o do Imobilizado': [f"EQUIPAMENTO TESTE {i}" for i in range(n_samples)],
        'Vida': np.random.choice([10, 15, 20, 25, 50], n_samples),
        'Valor AquisiÃ§Ã£o': [f"R$ {np.random.uniform(100, 50000):.2f}".replace('.', ',') for _ in range(n_samples)],
        'Valor ContÃ¡bil': [f"R$ {np.random.uniform(0, 30000):.2f}".replace('.', ',') for _ in range(n_samples)],
        'Item ConsumÃ­vel Similar': [f"CONSUMÃVEL SIMILAR {i}" for i in range(n_samples)],
        'Percentagem de Similaridade (%)': [f"{np.random.uniform(20, 85):.1f}%" for _ in range(n_samples)],
        'Data IncorporaÃ§Ã£o': pd.date_range('2010-01-01', '2025-01-01', periods=n_samples),
        'Justificativa de Reprova': np.random.choice([
            'RECLASSIFICAR: Alta similaridade (70%+)',
            'AVALIAR: Similaridade moderada (45%)',
            'MANTER: Similaridade baixa (25%)'
        ], n_samples, p=[0.25, 0.45, 0.3])
    }
    
    return pd.DataFrame(sample_data)

def main():
    st.title("ğŸ“Š Sistema de AnÃ¡lise de PatrimÃ´nio")
    st.markdown("---")
    
    # Aviso de recursos
    st.info("ğŸ’¡ **Otimizado para grande volume de dados** - Carregando apenas colunas essenciais para melhor performance")
    
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = PatrimonioAnalyzer()
    
    analyzer = st.session_state.analyzer
    
    with st.sidebar:
        st.header("âš™ï¸ ConfiguraÃ§Ã£o de Dados")
        
        load_mode = st.radio(
            "Modo de Carregamento:",
            ["ğŸ—‚ï¸ Pasta Local (Auto)", "ğŸ“¤ Upload Manual", "ğŸ”— URLs Remotas", "ğŸ¯ DemonstraÃ§Ã£o"],
            help="Escolha como carregar os dados"
        )
        
        st.markdown("---")
        
        if load_mode == "ğŸ—‚ï¸ Pasta Local (Auto)":
            st.info("ğŸ“‚ Carregamento automÃ¡tico da pasta 'data/'")
            
            folder_path = st.text_input("Caminho da pasta:", value="data")
            
            if st.button("ğŸ”„ Carregar Dados"):
                with st.spinner("Carregando arquivos..."):
                    if analyzer.load_data_from_folder(folder_path):
                        if analyzer.preprocess_data():
                            st.success(f"âœ“ Pronto!")
                        else:
                            st.error("Erro no processamento dos dados")
            
            if not analyzer.processed and 'auto_loaded' not in st.session_state:
                with st.spinner("Carregando dados automaticamente..."):
                    if analyzer.load_data_from_folder(folder_path):
                        if analyzer.preprocess_data():
                            st.session_state.auto_loaded = True
        
        elif load_mode == "ğŸ“¤ Upload Manual":
            uploaded_files = st.file_uploader(
                "Selecione os arquivos CSV",
                type=['csv'],
                accept_multiple_files=True,
                help="VocÃª pode selecionar mÃºltiplos arquivos CSV"
            )
            
            if uploaded_files:
                if st.button("ğŸ“¥ Carregar Arquivos"):
                    with st.spinner("Carregando arquivos..."):
                        if analyzer.load_data_from_upload(uploaded_files):
                            if analyzer.preprocess_data():
                                st.success(f"âœ“ {len(analyzer.data):,} registros carregados!")
                            else:
                                st.error("Erro no processamento dos dados")
        
        elif load_mode == "ğŸ”— URLs Remotas":
            st.info("ğŸ’¡ Suporta GitHub Raw, Google Drive pÃºblico, etc.")
            
            url_input = st.text_area(
                "URLs (uma por linha):",
                help="Cole as URLs dos arquivos CSV, uma por linha"
            )
            
            if st.button("ğŸŒ Carregar de URLs"):
                urls = [url.strip() for url in url_input.split('\n') if url.strip()]
                if urls:
                    with st.spinner("Baixando arquivos..."):
                        if analyzer.load_data_from_url(urls):
                            if analyzer.preprocess_data():
                                st.success(f"âœ“ {len(analyzer.data):,} registros carregados!")
                            else:
                                st.error("Erro no processamento dos dados")
        
        else:
            if st.button("ğŸ¯ Gerar Dados de DemonstraÃ§Ã£o"):
                with st.spinner("Criando dados de demonstraÃ§Ã£o..."):
                    analyzer.data = create_demo_data()
                    analyzer.preprocess_data()
                    st.success("âœ“ Dados de demonstraÃ§Ã£o carregados!")
    
    if not analyzer.processed:
        st.info("ğŸ‘† Configure o modo de carregamento na barra lateral para comeÃ§ar")
        return
    
    with st.sidebar:
        st.markdown("---")
        st.header("ğŸ”§ Filtros")
        
        localizacoes = ["Todas"] + sorted(analyzer.data['Regiao'].unique().tolist())
        regiao_filter = st.selectbox("LocalizaÃ§Ã£o", localizacoes)
        
        decisoes = ["Todas"] + sorted(analyzer.data['Decisao'].unique().tolist())
        decisao_filter = st.selectbox("DecisÃ£o", decisoes)
        
        anos = analyzer.data['Ano IncorporaÃ§Ã£o'].dropna()
        if len(anos) > 0:
            ano_min, ano_max = st.slider(
                "PerÃ­odo de IncorporaÃ§Ã£o",
                int(anos.min()),
                int(anos.max()),
                (int(anos.min()), int(anos.max()))
            )
        else:
            ano_min, ano_max = None, None
        
        similaridade_min = st.slider(
            "Similaridade MÃ­nima (%)",
            0, 100, 0
        )
        
        filtered_data = analyzer.get_filtered_data(
            regiao_filter, decisao_filter, ano_min, ano_max, similaridade_min
        )
    
    st.header("ğŸ“ˆ Resumo Executivo")
    
    metrics, regional_analysis = create_strategic_metrics(filtered_data)
    
    if metrics:
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("Total de Itens", f"{metrics['total_itens']:,}")
        
        with col2:
            st.metric("Para Reclassificar", f"{metrics['reclassificar']:,}",
                     f"{metrics['percentual_reclassificar']:.1f}%")
        
        with col3:
            st.metric("Para Avaliar", f"{metrics['avaliar']:,}",
                     f"{metrics['percentual_avaliar']:.1f}%")
        
        with col4:
            st.metric("Similaridade MÃ©dia", f"{metrics['similaridade_media']:.1f}%")
        
        with col5:
            st.metric("Idade MÃ©dia", f"{metrics['idade_media']:.1f} anos")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š VisÃ£o Geral", "ğŸ” AnÃ¡lise Detalhada", "ğŸ¢ AnÃ¡lise por LocalizaÃ§Ã£o", "ğŸ“‹ Dados e Export"])
    
    with tab1:
        st.subheader("VisÃ£o Geral dos Dados")
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig_pie = create_decision_pie_chart(filtered_data)
            st.plotly_chart(fig_pie, use_container_width=True)
            
            fig_hist = create_similarity_histogram(filtered_data)
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col2:
            fig_bar = create_regional_bar_chart(filtered_data)
            st.plotly_chart(fig_bar, use_container_width=True)
            
            fig_box = create_similarity_box_plot(filtered_data)
            st.plotly_chart(fig_box, use_container_width=True)
    
    with tab2:
        st.subheader("AnÃ¡lise Detalhada")
        
        fig_timeline = create_timeline_chart(filtered_data)
        st.plotly_chart(fig_timeline, use_container_width=True)
        
        st.subheader("ğŸ“… AnÃ¡lise de Idade dos Itens")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if 'Idade_Item' in filtered_data.columns:
                idade_reclas = filtered_data[filtered_data['Decisao'] == 'RECLASSIFICAR']['Idade_Item'].mean()
                st.metric("Idade MÃ©dia - RECLASSIFICAR", f"{idade_reclas:.1f} anos")
        
        with col2:
            if 'Idade_Item' in filtered_data.columns:
                idade_avaliar = filtered_data[filtered_data['Decisao'] == 'AVALIAR']['Idade_Item'].mean()
                st.metric("Idade MÃ©dia - AVALIAR", f"{idade_avaliar:.1f} anos")
        
        with col3:
            if 'Idade_Item' in filtered_data.columns:
                idade_manter = filtered_data[filtered_data['Decisao'] == 'MANTER']['Idade_Item'].mean()
                st.metric("Idade MÃ©dia - MANTER", f"{idade_manter:.1f} anos")
        
        if 'Idade_Item' in filtered_data.columns:
            fig_idade = px.box(
                filtered_data,
                x='Decisao',
                y='Idade_Item',
                title="DistribuiÃ§Ã£o de Idade por Tipo de DecisÃ£o",
                labels={'Idade_Item': 'Idade (anos)', 'Decisao': 'DecisÃ£o'},
                color='Decisao',
                color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
            )
            st.plotly_chart(fig_idade, use_container_width=True)
    
    with tab3:
        st.subheader("AnÃ¡lise por LocalizaÃ§Ã£o")
        
        if not regional_analysis.empty:
            regional_display = regional_analysis.copy()
            regional_display['EficiÃªncia_ReclassificaÃ§Ã£o'] = (
                regional_display['Itens_Reclassificar'] / 
                filtered_data.groupby('Regiao').size()
            ) * 100
            
            regional_display = regional_display.round(2)
            regional_display.columns = ['Itens p/ Reclassificar', 'Similaridade MÃ©dia (%)', 'EficiÃªncia (%)']
            
            st.dataframe(regional_display.sort_values('EficiÃªncia (%)', ascending=False))
            
            fig = px.scatter(
                x=regional_display['Similaridade MÃ©dia (%)'],
                y=regional_display['EficiÃªncia (%)'],
                size=regional_display['Itens p/ Reclassificar'],
                hover_name=regional_display.index,
                title="EficiÃªncia por LocalizaÃ§Ã£o: Similaridade vs % ReclassificaÃ§Ã£o",
                labels={
                    'x': 'Similaridade MÃ©dia (%)',
                    'y': 'EficiÃªncia na ReclassificaÃ§Ã£o (%)'
                }
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    with tab4:
        st.subheader("Dados e ExportaÃ§Ã£o")
        
        with st.expander("ğŸ” DiagnÃ³stico de Idade dos Itens"):
            if 'Ano IncorporaÃ§Ã£o' in filtered_data.columns and 'Idade_Item' in filtered_data.columns:
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Ano MÃ©dio de IncorporaÃ§Ã£o", 
                             f"{filtered_data['Ano IncorporaÃ§Ã£o'].mean():.0f}")
                    st.metric("Ano MÃ­nimo", 
                             f"{filtered_data['Ano IncorporaÃ§Ã£o'].min():.0f}")
                    st.metric("Ano MÃ¡ximo", 
                             f"{filtered_data['Ano IncorporaÃ§Ã£o'].max():.0f}")
                
                with col2:
                    st.metric("Idade MÃ©dia Calculada", 
                             f"{filtered_data['Idade_Item'].mean():.1f} anos")
                    st.metric("Idade MÃ­nima", 
                             f"{filtered_data['Idade_Item'].min():.0f} anos")
                    st.metric("Idade MÃ¡xima", 
                             f"{filtered_data['Idade_Item'].max():.0f} anos")
                
                with col3:
                    total_registros = len(filtered_data)
                    registros_com_data = filtered_data['Ano IncorporaÃ§Ã£o'].notna().sum()
                    registros_sem_data = total_registros - registros_com_data
                    
                    st.metric("Total de Registros", f"{total_registros:,}")
                    st.metric("Com Data", f"{registros_com_data:,}")
                    st.metric("Sem Data", f"{registros_sem_data:,}")
                
                st.write("**DistribuiÃ§Ã£o por DÃ©cada de IncorporaÃ§Ã£o:**")
                filtered_data_copy = filtered_data.copy()
                filtered_data_copy['Decada'] = (filtered_data_copy['Ano IncorporaÃ§Ã£o'] // 10 * 10).astype('Int64')
                decada_counts = filtered_data_copy['Decada'].value_counts().sort_index()
                
                fig_decada = px.bar(
                    x=decada_counts.index.astype(str),
                    y=decada_counts.values,
                    title="Quantidade de Itens por DÃ©cada",
                    labels={'x': 'DÃ©cada', 'y': 'Quantidade de Itens'}
                )
                st.plotly_chart(fig_decada, use_container_width=True)
        
        st.write(f"**Mostrando {len(filtered_data):,} registros filtrados:**")
        
        # Limita visualizaÃ§Ã£o para evitar problemas de performance
        max_display = 10000
        if len(filtered_data) > max_display:
            st.warning(f"âš ï¸ Mostrando apenas {max_display:,} registros de {len(filtered_data):,} para melhor performance")
            display_data = filtered_data.head(max_display)
        else:
            display_data = filtered_data
        
        show_columns = st.multiselect(
            "Selecione as colunas para exibir:",
            display_data.columns.tolist(),
            default=['LocalizaÃ§Ã£o', 'InventÃ¡rio', 'DenominaÃ§Ã£o do Imobilizado', 'Item ConsumÃ­vel Similar', 'Decisao']
        )
        
        if show_columns:
            st.dataframe(display_data[show_columns], height=400)
        
        st.subheader("ğŸ’¾ Exportar Dados")
        
        col1, col2 = st.columns(2)
        
        with col1:
            csv = filtered_data.to_csv(index=False)
            st.download_button(
                label="ğŸ“„ Baixar CSV",
                data=csv,
                file_name=f"patrimonio_filtrado_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv"
            )
        
        with col2:
            excel_data = export_to_excel(filtered_data)
            st.download_button(
                label="ğŸ“Š Baixar Excel",
                data=excel_data,
                file_name=f"patrimonio_analise_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

if __name__ == "__main__":
    main()






# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.express as px
# import plotly.graph_objects as go
# from plotly.subplots import make_subplots
# from datetime import datetime, date
# import re
# from io import BytesIO
# import base64
# import os
# from pathlib import Path

# # ConfiguraÃ§Ã£o da pÃ¡gina
# st.set_page_config(
#     page_title="AnÃ¡lise de PatrimÃ´nio",
#     page_icon="ğŸ“Š",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )

# class PatrimonioAnalyzer:
#     def __init__(self):
#         self.data = None
#         self.processed = False
    
#     def load_data_from_folder(self, folder_path='data'):
#         """Carrega automaticamente todos os CSVs de uma pasta"""
#         dataframes = []
#         folder = Path(folder_path)
        
#         if not folder.exists():
#             st.warning(f"âš ï¸ Pasta '{folder_path}' nÃ£o encontrada. Criando pasta...")
#             folder.mkdir(exist_ok=True)
#             return False
        
#         csv_files = list(folder.glob('*.csv'))
        
#         if not csv_files:
#             st.warning(f"âš ï¸ Nenhum arquivo CSV encontrado em '{folder_path}'")
#             return False
        
#         st.info(f"ğŸ“‚ Carregando {len(csv_files)} arquivo(s) CSV da pasta '{folder_path}'...")
        
#         for csv_file in csv_files:
#             try:
#                 df = pd.read_csv(csv_file, encoding='utf-8')
#                 df['arquivo_origem'] = csv_file.name
#                 dataframes.append(df)
#                 st.success(f"âœ“ {csv_file.name}: {len(df)} registros carregados")
#             except UnicodeDecodeError:
#                 try:
#                     df = pd.read_csv(csv_file, encoding='latin-1')
#                     df['arquivo_origem'] = csv_file.name
#                     dataframes.append(df)
#                     st.success(f"âœ“ {csv_file.name}: {len(df)} registros carregados (latin-1)")
#                 except Exception as e:
#                     st.error(f"âœ— Erro ao carregar {csv_file.name}: {str(e)}")
#             except Exception as e:
#                 st.error(f"âœ— Erro ao carregar {csv_file.name}: {str(e)}")
        
#         if dataframes:
#             self.data = pd.concat(dataframes, ignore_index=True)
#             return True
#         return False
    
#     def load_data_from_upload(self, uploaded_files):
#         """Carrega dados dos arquivos CSV enviados via upload"""
#         dataframes = []
        
#         for uploaded_file in uploaded_files:
#             try:
#                 df = pd.read_csv(uploaded_file, encoding='utf-8')
#                 df['arquivo_origem'] = uploaded_file.name
#                 dataframes.append(df)
#                 st.success(f"âœ“ {uploaded_file.name}: {len(df)} registros carregados")
#             except Exception as e:
#                 st.error(f"âœ— Erro ao carregar {uploaded_file.name}: {str(e)}")
        
#         if dataframes:
#             self.data = pd.concat(dataframes, ignore_index=True)
#             return True
#         return False
    
#     def load_data_from_url(self, urls):
#         """Carrega dados de URLs (GitHub, Google Drive, etc)"""
#         dataframes = []
        
#         for url in urls:
#             try:
#                 if 'drive.google.com' in url:
#                     file_id = url.split('/d/')[1].split('/')[0]
#                     url = f'https://drive.google.com/uc?id={file_id}'
                
#                 df = pd.read_csv(url)
#                 df['arquivo_origem'] = f"URL_{len(dataframes)+1}"
#                 dataframes.append(df)
#                 st.success(f"âœ“ URL {len(dataframes)}: {len(df)} registros carregados")
#             except Exception as e:
#                 st.error(f"âœ— Erro ao carregar URL: {str(e)}")
        
#         if dataframes:
#             self.data = pd.concat(dataframes, ignore_index=True)
#             return True
#         return False
    
#     def preprocess_data(self):
#         """Preprocessa os dados"""
#         if self.data is None:
#             return False
        
#         self.data.columns = self.data.columns.str.strip()
        
#         if 'Data IncorporaÃ§Ã£o' in self.data.columns:
#             self.data['Data IncorporaÃ§Ã£o'] = pd.to_datetime(self.data['Data IncorporaÃ§Ã£o'], errors='coerce')
#             self.data['Ano IncorporaÃ§Ã£o'] = self.data['Data IncorporaÃ§Ã£o'].dt.year
#             # Calcula idade dos itens
#             self.data['Idade_Item'] = 2025 - self.data['Ano IncorporaÃ§Ã£o']
        
#         if 'Percentagem de Similaridade (%)' in self.data.columns:
#             self.data['Similaridade_Num'] = pd.to_numeric(
#                 self.data['Percentagem de Similaridade (%)'].str.replace('%', ''), 
#                 errors='coerce'
#             )
        
#         if 'Valor AquisiÃ§Ã£o' in self.data.columns:
#             self.data['Valor_Aquisicao_Num'] = pd.to_numeric(
#                 self.data['Valor AquisiÃ§Ã£o'].str.replace('R$', '').str.replace('.', '').str.replace(',', '.').str.strip(),
#                 errors='coerce'
#             )
        
#         if 'Valor ContÃ¡bil' in self.data.columns:
#             self.data['Valor_Contabil_Num'] = pd.to_numeric(
#                 self.data['Valor ContÃ¡bil'].str.replace('R$', '').str.replace('.', '').str.replace(',', '.').str.strip(),
#                 errors='coerce'
#             )
        
#         if 'Vida' in self.data.columns:
#             self.data['Vida_Num'] = pd.to_numeric(self.data['Vida'], errors='coerce')
        
#         if 'Justificativa de Reprova' in self.data.columns:
#             self.data['Decisao'] = self.data['Justificativa de Reprova'].apply(self._categorizar_decisao)
#             self.data['Categoria_Similaridade'] = self.data['Similaridade_Num'].apply(self._categorizar_similaridade)
        
#         if 'LocalizaÃ§Ã£o' in self.data.columns:
#             self.data['Regiao'] = self.data['LocalizaÃ§Ã£o'].apply(self._extrair_regiao)
        
#         self.processed = True
#         return True
    
#     def _categorizar_decisao(self, justificativa):
#         if pd.isna(justificativa):
#             return "Sem Categoria"
        
#         justificativa = str(justificativa).upper()
        
#         if "RECLASSIFICAR" in justificativa:
#             return "RECLASSIFICAR"
#         elif "AVALIAR" in justificativa:
#             return "AVALIAR"
#         elif "MANTER" in justificativa:
#             return "MANTER"
#         else:
#             return "OUTROS"
    
#     def _categorizar_similaridade(self, similaridade):
#         if pd.isna(similaridade):
#             return "Sem Dados"
        
#         if similaridade >= 70:
#             return "Muito Alta (â‰¥70%)"
#         elif similaridade >= 50:
#             return "Alta (50-69%)"
#         elif similaridade >= 35:
#             return "Moderada (35-49%)"
#         else:
#             return "Baixa (<35%)"
    
#     def _extrair_regiao(self, localizacao):
#         if pd.isna(localizacao):
#             return "NÃ£o Informado"
        
#         return str(localizacao).strip()
    
#     def get_filtered_data(self, regiao_filter=None, decisao_filter=None, 
#                          ano_min=None, ano_max=None, similaridade_min=None):
#         """Retorna dados filtrados"""
#         if not self.processed:
#             return pd.DataFrame()
        
#         filtered = self.data.copy()
        
#         if regiao_filter and regiao_filter != "Todas":
#             filtered = filtered[filtered['Regiao'] == regiao_filter]
        
#         if decisao_filter and decisao_filter != "Todas":
#             filtered = filtered[filtered['Decisao'] == decisao_filter]
        
#         if ano_min is not None:
#             filtered = filtered[filtered['Ano IncorporaÃ§Ã£o'] >= ano_min]
        
#         if ano_max is not None:
#             filtered = filtered[filtered['Ano IncorporaÃ§Ã£o'] <= ano_max]
        
#         if similaridade_min is not None:
#             filtered = filtered[filtered['Similaridade_Num'] >= similaridade_min]
        
#         return filtered

# def create_decision_pie_chart(data):
#     """GrÃ¡fico de pizza das decisÃµes"""
#     decisoes = data['Decisao'].value_counts()
    
#     fig = px.pie(
#         values=decisoes.values, 
#         names=decisoes.index,
#         title="DistribuiÃ§Ã£o de DecisÃµes",
#         color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
#     )
    
#     fig.update_traces(textposition='inside', textinfo='percent+label')
#     return fig

# def create_similarity_histogram(data):
#     """Histograma de similaridade"""
#     fig = px.histogram(
#         data, 
#         x='Similaridade_Num',
#         nbins=20,
#         title="DistribuiÃ§Ã£o de Similaridade (%)",
#         labels={'Similaridade_Num': 'Similaridade (%)', 'count': 'Quantidade'}
#     )
    
#     fig.update_layout(showlegend=False)
#     return fig

# def create_regional_bar_chart(data):
#     """GrÃ¡fico de barras por localizaÃ§Ã£o"""
#     localizacao_counts = data['Regiao'].value_counts()
    
#     fig = px.bar(
#         x=localizacao_counts.index,
#         y=localizacao_counts.values,
#         title="Itens por LocalizaÃ§Ã£o",
#         labels={'x': 'LocalizaÃ§Ã£o', 'y': 'Quantidade'}
#     )
    
#     fig.update_layout(
#         showlegend=False,
#         xaxis=dict(tickangle=45)
#     )
#     return fig

# def create_similarity_box_plot(data):
#     """Box plot de similaridade por decisÃ£o"""
#     fig = px.box(
#         data,
#         x='Decisao',
#         y='Similaridade_Num',
#         title="Similaridade por Tipo de DecisÃ£o",
#         labels={'Similaridade_Num': 'Similaridade (%)', 'Decisao': 'DecisÃ£o'}
#     )
    
#     return fig

# def create_timeline_chart(data):
#     """GrÃ¡fico temporal"""
#     timeline = data.groupby(['Ano IncorporaÃ§Ã£o', 'Decisao']).size().unstack(fill_value=0)
    
#     fig = go.Figure()
    
#     colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
#     for i, decisao in enumerate(timeline.columns):
#         fig.add_trace(
#             go.Scatter(
#                 x=timeline.index,
#                 y=timeline[decisao],
#                 mode='lines+markers',
#                 name=decisao,
#                 line=dict(color=colors[i % len(colors)])
#             )
#         )
    
#     fig.update_layout(
#         title="Timeline de IncorporaÃ§Ãµes por DecisÃ£o",
#         xaxis_title="Ano",
#         yaxis_title="Quantidade",
#         hovermode='x unified'
#     )
    
#     return fig

# def create_strategic_metrics(data):
#     """MÃ©tricas estratÃ©gicas"""
#     total_itens = len(data)
    
#     if total_itens == 0:
#         return {}, {}
    
#     decisoes = data['Decisao'].value_counts()
#     reclassificar = decisoes.get('RECLASSIFICAR', 0)
#     avaliar = decisoes.get('AVALIAR', 0)
#     manter = decisoes.get('MANTER', 0)
    
#     similaridade_stats = data['Similaridade_Num'].describe()
    
#     # Calcula idade mÃ©dia dos itens
#     idade_media = data['Idade_Item'].mean() if 'Idade_Item' in data.columns else 0
    
#     metrics = {
#         'total_itens': total_itens,
#         'reclassificar': reclassificar,
#         'avaliar': avaliar,
#         'manter': manter,
#         'percentual_reclassificar': (reclassificar / total_itens) * 100,
#         'percentual_avaliar': (avaliar / total_itens) * 100,
#         'similaridade_media': similaridade_stats.get('mean', 0),
#         'alta_similaridade': len(data[data['Similaridade_Num'] >= 70]),
#         'idade_media': idade_media
#     }
    
#     regional_analysis = data.groupby('Regiao').agg({
#         'Decisao': lambda x: (x == 'RECLASSIFICAR').sum(),
#         'Similaridade_Num': 'mean'
#     }).round(2)
    
#     regional_analysis.columns = ['Itens_Reclassificar', 'Similaridade_Media']
    
#     return metrics, regional_analysis

# def export_to_excel(data):
#     """Exporta dados para Excel"""
#     output = BytesIO()
    
#     with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
#         data.to_excel(writer, sheet_name='Dados_Completos', index=False)
        
#         decisao_summary = data.groupby('Decisao').agg({
#             'Similaridade_Num': ['count', 'mean', 'min', 'max'],
#             'Ano IncorporaÃ§Ã£o': ['min', 'max']
#         }).round(2)
#         decisao_summary.to_excel(writer, sheet_name='Resumo_Decisoes')
        
#         regional_summary = data.groupby('Regiao').agg({
#             'Decisao': lambda x: x.value_counts().to_dict(),
#             'Similaridade_Num': ['count', 'mean']
#         })
#         regional_summary.to_excel(writer, sheet_name='Resumo_Regional')
    
#     return output.getvalue()

# def create_demo_data():
#     """Cria dados de demonstraÃ§Ã£o"""
#     np.random.seed(42)
#     n_samples = 1000
    
#     sample_data = {
#         'LocalizaÃ§Ã£o': np.random.choice(['1.01 BRÃS', '1.02 VILA ALPINA', '2.01 SANTOS', 'SENAI SEDE', '3.01 TAUBATÃ‰'], n_samples),
#         'InventÃ¡rio': range(1000000, 1000000 + n_samples),
#         'DenominaÃ§Ã£o do Imobilizado': [f"EQUIPAMENTO TESTE {i}" for i in range(n_samples)],
#         'Vida': np.random.choice([10, 15, 20, 25, 50], n_samples),
#         'Valor AquisiÃ§Ã£o': [f"R$ {np.random.uniform(100, 50000):.2f}".replace('.', ',') for _ in range(n_samples)],
#         'Valor ContÃ¡bil': [f"R$ {np.random.uniform(0, 30000):.2f}".replace('.', ',') for _ in range(n_samples)],
#         'Item ConsumÃ­vel Similar': [f"CONSUMÃVEL SIMILAR {i}" for i in range(n_samples)],
#         'Percentagem de Similaridade (%)': [f"{np.random.uniform(20, 85):.1f}%" for _ in range(n_samples)],
#         'Data IncorporaÃ§Ã£o': pd.date_range('2010-01-01', '2025-01-01', periods=n_samples),
#         'Justificativa de Reprova': np.random.choice([
#             'RECLASSIFICAR: Alta similaridade (70%+)',
#             'AVALIAR: Similaridade moderada (45%)',
#             'MANTER: Similaridade baixa (25%)'
#         ], n_samples, p=[0.25, 0.45, 0.3])
#     }
    
#     return pd.DataFrame(sample_data)

# def main():
#     st.title("ğŸ“Š Sistema de AnÃ¡lise de PatrimÃ´nio")
#     st.markdown("---")
    
#     if 'analyzer' not in st.session_state:
#         st.session_state.analyzer = PatrimonioAnalyzer()
    
#     analyzer = st.session_state.analyzer
    
#     with st.sidebar:
#         st.header("âš™ï¸ ConfiguraÃ§Ã£o de Dados")
        
#         load_mode = st.radio(
#             "Modo de Carregamento:",
#             ["ğŸ—‚ï¸ Pasta Local (Auto)", "ğŸ“¤ Upload Manual", "ğŸ”— URLs Remotas", "ğŸ¯ DemonstraÃ§Ã£o"],
#             help="Escolha como carregar os dados"
#         )
        
#         st.markdown("---")
        
#         if load_mode == "ğŸ—‚ï¸ Pasta Local (Auto)":
#             st.info("ğŸ“‚ Carregamento automÃ¡tico da pasta 'data/'")
            
#             folder_path = st.text_input("Caminho da pasta:", value="data")
            
#             if st.button("ğŸ”„ Carregar Dados"):
#                 with st.spinner("Carregando arquivos..."):
#                     if analyzer.load_data_from_folder(folder_path):
#                         if analyzer.preprocess_data():
#                             st.success(f"âœ“ {len(analyzer.data)} registros carregados!")
#                         else:
#                             st.error("Erro no processamento dos dados")
            
#             if not analyzer.processed and 'auto_loaded' not in st.session_state:
#                 with st.spinner("Carregando dados automaticamente..."):
#                     if analyzer.load_data_from_folder(folder_path):
#                         if analyzer.preprocess_data():
#                             st.success(f"âœ“ {len(analyzer.data)} registros carregados automaticamente!")
#                             st.session_state.auto_loaded = True
        
#         elif load_mode == "ğŸ“¤ Upload Manual":
#             uploaded_files = st.file_uploader(
#                 "Selecione os arquivos CSV",
#                 type=['csv'],
#                 accept_multiple_files=True,
#                 help="VocÃª pode selecionar atÃ© 12 arquivos CSV"
#             )
            
#             if uploaded_files:
#                 if st.button("ğŸ“¥ Carregar Arquivos"):
#                     with st.spinner("Carregando arquivos..."):
#                         if analyzer.load_data_from_upload(uploaded_files):
#                             if analyzer.preprocess_data():
#                                 st.success(f"âœ“ {len(analyzer.data)} registros carregados!")
#                             else:
#                                 st.error("Erro no processamento dos dados")
        
#         elif load_mode == "ğŸ”— URLs Remotas":
#             st.info("ğŸ’¡ Suporta GitHub Raw, Google Drive pÃºblico, etc.")
            
#             url_input = st.text_area(
#                 "URLs (uma por linha):",
#                 help="Cole as URLs dos arquivos CSV, uma por linha"
#             )
            
#             if st.button("ğŸŒ Carregar de URLs"):
#                 urls = [url.strip() for url in url_input.split('\n') if url.strip()]
#                 if urls:
#                     with st.spinner("Baixando arquivos..."):
#                         if analyzer.load_data_from_url(urls):
#                             if analyzer.preprocess_data():
#                                 st.success(f"âœ“ {len(analyzer.data)} registros carregados!")
#                             else:
#                                 st.error("Erro no processamento dos dados")
        
#         else:
#             if st.button("ğŸ¯ Gerar Dados de DemonstraÃ§Ã£o"):
#                 with st.spinner("Criando dados de demonstraÃ§Ã£o..."):
#                     analyzer.data = create_demo_data()
#                     analyzer.preprocess_data()
#                     st.success("âœ“ Dados de demonstraÃ§Ã£o carregados!")
    
#     if not analyzer.processed:
#         st.info("ğŸ‘† Configure o modo de carregamento na barra lateral para comeÃ§ar")
        
#         with st.expander("ğŸ“– Como usar cada modo"):
#             st.markdown("""
#             ### ğŸ—‚ï¸ Pasta Local (Auto)
#             - **Para Streamlit Cloud**: Crie uma pasta `data/` no seu repositÃ³rio
#             - Coloque seus arquivos CSV dentro dessa pasta
#             - Os dados serÃ£o carregados automaticamente no deploy
            
#             ### ğŸ“¤ Upload Manual
#             - Use para testes locais ou dados temporÃ¡rios
#             - Arraste e solte seus CSVs
            
#             ### ğŸ”— URLs Remotas
#             - **GitHub**: Use URL raw (ex: `https://raw.githubusercontent.com/user/repo/main/data.csv`)
#             - **Google Drive**: Compartilhe o arquivo publicamente e use o link
#             - **Outros**: Qualquer URL pÃºblica de CSV
            
#             ### ğŸ¯ DemonstraÃ§Ã£o
#             - Gera 1000 registros de exemplo para testar a aplicaÃ§Ã£o
#             """)
        
#         return
    
#     with st.sidebar:
#         st.markdown("---")
#         st.header("ğŸ”§ Filtros")
        
#         localizacoes = ["Todas"] + sorted(analyzer.data['Regiao'].unique().tolist())
#         regiao_filter = st.selectbox("LocalizaÃ§Ã£o", localizacoes)
        
#         decisoes = ["Todas"] + sorted(analyzer.data['Decisao'].unique().tolist())
#         decisao_filter = st.selectbox("DecisÃ£o", decisoes)
        
#         anos = analyzer.data['Ano IncorporaÃ§Ã£o'].dropna()
#         if len(anos) > 0:
#             ano_min, ano_max = st.slider(
#                 "PerÃ­odo de IncorporaÃ§Ã£o",
#                 int(anos.min()),
#                 int(anos.max()),
#                 (int(anos.min()), int(anos.max()))
#             )
#         else:
#             ano_min, ano_max = None, None
        
#         similaridade_min = st.slider(
#             "Similaridade MÃ­nima (%)",
#             0, 100, 0
#         )
        
#         filtered_data = analyzer.get_filtered_data(
#             regiao_filter, decisao_filter, ano_min, ano_max, similaridade_min
#         )
    
#     st.header("ğŸ“ˆ Resumo Executivo")
    
#     metrics, regional_analysis = create_strategic_metrics(filtered_data)
    
#     if metrics:
#         col1, col2, col3, col4, col5 = st.columns(5)
        
#         with col1:
#             st.metric(
#                 "Total de Itens",
#                 f"{metrics['total_itens']:,}",
#                 help="NÃºmero total de itens na anÃ¡lise"
#             )
        
#         with col2:
#             st.metric(
#                 "Para Reclassificar",
#                 f"{metrics['reclassificar']:,}",
#                 f"{metrics['percentual_reclassificar']:.1f}%",
#                 help="Itens recomendados para reclassificaÃ§Ã£o"
#             )
        
#         with col3:
#             st.metric(
#                 "Para Avaliar",
#                 f"{metrics['avaliar']:,}",
#                 f"{metrics['percentual_avaliar']:.1f}%",
#                 help="Itens que necessitam avaliaÃ§Ã£o adicional"
#             )
        
#         with col4:
#             st.metric(
#                 "Similaridade MÃ©dia",
#                 f"{metrics['similaridade_media']:.1f}%",
#                 help="Percentual mÃ©dio de similaridade com consumÃ­veis"
#             )
        
#         with col5:
#             st.metric(
#                 "Idade MÃ©dia",
#                 f"{metrics['idade_media']:.1f} anos",
#                 help="Idade mÃ©dia dos itens patrimoniais (2025 - Ano de IncorporaÃ§Ã£o)"
#             )
    
#     tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š VisÃ£o Geral", "ğŸ” AnÃ¡lise Detalhada", "ğŸ¢ AnÃ¡lise por LocalizaÃ§Ã£o", "ğŸ“‹ Dados e Export"])
    
#     with tab1:
#         st.subheader("VisÃ£o Geral dos Dados")
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             fig_pie = create_decision_pie_chart(filtered_data)
#             st.plotly_chart(fig_pie, use_container_width=True)
            
#             fig_hist = create_similarity_histogram(filtered_data)
#             st.plotly_chart(fig_hist, use_container_width=True)
        
#         with col2:
#             fig_bar = create_regional_bar_chart(filtered_data)
#             st.plotly_chart(fig_bar, use_container_width=True)
            
#             fig_box = create_similarity_box_plot(filtered_data)
#             st.plotly_chart(fig_box, use_container_width=True)
    
#     with tab2:
#         st.subheader("AnÃ¡lise Detalhada")
        
#         fig_timeline = create_timeline_chart(filtered_data)
#         st.plotly_chart(fig_timeline, use_container_width=True)
        
#         # AnÃ¡lise de Idade por DecisÃ£o
#         st.subheader("ğŸ“… AnÃ¡lise de Idade dos Itens")
        
#         col1, col2, col3 = st.columns(3)
        
#         with col1:
#             if 'Idade_Item' in filtered_data.columns:
#                 idade_reclassificar = filtered_data[filtered_data['Decisao'] == 'RECLASSIFICAR']['Idade_Item'].mean()
#                 st.metric(
#                     "Idade MÃ©dia - RECLASSIFICAR",
#                     f"{idade_reclassificar:.1f} anos",
#                     help="Idade mÃ©dia dos itens para reclassificar"
#                 )
        
#         with col2:
#             if 'Idade_Item' in filtered_data.columns:
#                 idade_avaliar = filtered_data[filtered_data['Decisao'] == 'AVALIAR']['Idade_Item'].mean()
#                 st.metric(
#                     "Idade MÃ©dia - AVALIAR",
#                     f"{idade_avaliar:.1f} anos",
#                     help="Idade mÃ©dia dos itens para avaliar"
#                 )
        
#         with col3:
#             if 'Idade_Item' in filtered_data.columns:
#                 idade_manter = filtered_data[filtered_data['Decisao'] == 'MANTER']['Idade_Item'].mean()
#                 st.metric(
#                     "Idade MÃ©dia - MANTER",
#                     f"{idade_manter:.1f} anos",
#                     help="Idade mÃ©dia dos itens para manter"
#                 )
        
#         # Box plot de idade por decisÃ£o
#         if 'Idade_Item' in filtered_data.columns:
#             fig_idade = px.box(
#                 filtered_data,
#                 x='Decisao',
#                 y='Idade_Item',
#                 title="DistribuiÃ§Ã£o de Idade por Tipo de DecisÃ£o",
#                 labels={'Idade_Item': 'Idade (anos)', 'Decisao': 'DecisÃ£o'},
#                 color='Decisao',
#                 color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
#             )
#             st.plotly_chart(fig_idade, use_container_width=True)
        
#         st.subheader("âš ï¸ Itens de Alta Prioridade")
        
#         alta_similaridade = filtered_data[filtered_data['Similaridade_Num'] >= 70]
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             st.info(f"**{len(alta_similaridade):,} itens** com similaridade â‰¥ 70%")
            
#             if len(alta_similaridade) > 0:
#                 top_regioes = alta_similaridade['Regiao'].value_counts().head()
#                 st.write("**LocalizaÃ§Ãµes com mais itens de alta similaridade:**")
#                 for localizacao, count in top_regioes.items():
#                     st.write(f"â€¢ {localizacao}: {count} itens")
        
#         with col2:
#             reclassificacao_imediata = filtered_data[filtered_data['Decisao'] == 'RECLASSIFICAR']
#             st.warning(f"**{len(reclassificacao_imediata):,} itens** para reclassificaÃ§Ã£o imediata")
            
#             if len(reclassificacao_imediata) > 0:
#                 similaridade_media = reclassificacao_imediata['Similaridade_Num'].mean()
#                 st.write(f"**Similaridade mÃ©dia:** {similaridade_media:.1f}%")
#                 if 'Idade_Item' in reclassificacao_imediata.columns:
#                     idade_media_reclas = reclassificacao_imediata['Idade_Item'].mean()
#                     st.write(f"**Idade mÃ©dia:** {idade_media_reclas:.1f} anos")
    
#     with tab3:
#         st.subheader("AnÃ¡lise por LocalizaÃ§Ã£o")
        
#         if not regional_analysis.empty:
#             st.write("**Resumo por LocalizaÃ§Ã£o:**")
            
#             regional_display = regional_analysis.copy()
#             regional_display['EficiÃªncia_ReclassificaÃ§Ã£o'] = (
#                 regional_display['Itens_Reclassificar'] / 
#                 filtered_data.groupby('Regiao').size()
#             ) * 100
            
#             regional_display = regional_display.round(2)
#             regional_display.columns = ['Itens p/ Reclassificar', 'Similaridade MÃ©dia (%)', 'EficiÃªncia (%)']
            
#             st.dataframe(regional_display.sort_values('EficiÃªncia (%)', ascending=False))
            
#             fig = px.scatter(
#                 x=regional_display['Similaridade MÃ©dia (%)'],
#                 y=regional_display['EficiÃªncia (%)'],
#                 size=regional_display['Itens p/ Reclassificar'],
#                 hover_name=regional_display.index,
#                 title="EficiÃªncia por LocalizaÃ§Ã£o: Similaridade vs % ReclassificaÃ§Ã£o",
#                 labels={
#                     'x': 'Similaridade MÃ©dia (%)',
#                     'y': 'EficiÃªncia na ReclassificaÃ§Ã£o (%)'
#                 }
#             )
            
#             st.plotly_chart(fig, use_container_width=True)
    
#     with tab4:
#         st.subheader("Dados e ExportaÃ§Ã£o")
        
#         # DiagnÃ³stico de Idade
#         with st.expander("ğŸ” DiagnÃ³stico de Idade dos Itens"):
#             if 'Ano IncorporaÃ§Ã£o' in filtered_data.columns and 'Idade_Item' in filtered_data.columns:
#                 col1, col2, col3 = st.columns(3)
                
#                 with col1:
#                     st.metric("Ano MÃ©dio de IncorporaÃ§Ã£o", 
#                              f"{filtered_data['Ano IncorporaÃ§Ã£o'].mean():.0f}")
#                     st.metric("Ano MÃ­nimo", 
#                              f"{filtered_data['Ano IncorporaÃ§Ã£o'].min():.0f}")
#                     st.metric("Ano MÃ¡ximo", 
#                              f"{filtered_data['Ano IncorporaÃ§Ã£o'].max():.0f}")
                
#                 with col2:
#                     st.metric("Idade MÃ©dia Calculada", 
#                              f"{filtered_data['Idade_Item'].mean():.1f} anos")
#                     st.metric("Idade MÃ­nima", 
#                              f"{filtered_data['Idade_Item'].min():.0f} anos")
#                     st.metric("Idade MÃ¡xima", 
#                              f"{filtered_data['Idade_Item'].max():.0f} anos")
                
#                 with col3:
#                     total_registros = len(filtered_data)
#                     registros_com_data = filtered_data['Ano IncorporaÃ§Ã£o'].notna().sum()
#                     registros_sem_data = total_registros - registros_com_data
                    
#                     st.metric("Total de Registros", f"{total_registros:,}")
#                     st.metric("Com Data", f"{registros_com_data:,}")
#                     st.metric("Sem Data", f"{registros_sem_data:,}", 
#                              delta=f"{(registros_sem_data/total_registros*100):.1f}%" if total_registros > 0 else "0%")
                
#                 # DistribuiÃ§Ã£o por dÃ©cada
#                 st.write("**DistribuiÃ§Ã£o por DÃ©cada de IncorporaÃ§Ã£o:**")
#                 filtered_data['Decada'] = (filtered_data['Ano IncorporaÃ§Ã£o'] // 10 * 10).astype('Int64')
#                 decada_counts = filtered_data['Decada'].value_counts().sort_index()
                
#                 fig_decada = px.bar(
#                     x=decada_counts.index.astype(str),
#                     y=decada_counts.values,
#                     title="Quantidade de Itens por DÃ©cada",
#                     labels={'x': 'DÃ©cada', 'y': 'Quantidade de Itens'}
#                 )
#                 st.plotly_chart(fig_decada, use_container_width=True)
        
#         st.write(f"**Mostrando {len(filtered_data):,} registros filtrados:**")
        
#         show_columns = st.multiselect(
#             "Selecione as colunas para exibir:",
#             filtered_data.columns.tolist(),
#             default=['LocalizaÃ§Ã£o', 'InventÃ¡rio', 'DenominaÃ§Ã£o do Imobilizado', 'Item ConsumÃ­vel Similar', 'Decisao']
#         )
        
#         if show_columns:
#             st.dataframe(filtered_data[show_columns])
        
#         st.subheader("ğŸ’¾ Exportar Dados")
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             csv = filtered_data.to_csv(index=False)
#             st.download_button(
#                 label="ğŸ“„ Baixar CSV",
#                 data=csv,
#                 file_name=f"patrimonio_filtrado_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
#                 mime="text/csv"
#             )
        
#         with col2:
#             excel_data = export_to_excel(filtered_data)
#             st.download_button(
#                 label="ğŸ“Š Baixar Excel",
#                 data=excel_data,
#                 file_name=f"patrimonio_analise_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
#                 mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#             )

# if __name__ == "__main__":
#     main()



# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.express as px
# import plotly.graph_objects as go
# from plotly.subplots import make_subplots
# from datetime import datetime, date
# import re
# from io import BytesIO
# import base64
# import os
# from pathlib import Path

# # ConfiguraÃ§Ã£o da pÃ¡gina
# st.set_page_config(
#     page_title="AnÃ¡lise de PatrimÃ´nio",
#     page_icon="ğŸ“Š",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )

# class PatrimonioAnalyzer:
#     def __init__(self):
#         self.data = None
#         self.processed = False
    
#     def load_data_from_folder(self, folder_path='data'):
#         """Carrega automaticamente todos os CSVs de uma pasta"""
#         dataframes = []
#         folder = Path(folder_path)
        
#         if not folder.exists():
#             st.warning(f"âš ï¸ Pasta '{folder_path}' nÃ£o encontrada. Criando pasta...")
#             folder.mkdir(exist_ok=True)
#             return False
        
#         csv_files = list(folder.glob('*.csv'))
        
#         if not csv_files:
#             st.warning(f"âš ï¸ Nenhum arquivo CSV encontrado em '{folder_path}'")
#             return False
        
#         st.info(f"ğŸ“‚ Carregando {len(csv_files)} arquivo(s) CSV da pasta '{folder_path}'...")
        
#         for csv_file in csv_files:
#             try:
#                 df = pd.read_csv(csv_file, encoding='utf-8')
#                 df['arquivo_origem'] = csv_file.name
#                 dataframes.append(df)
#                 st.success(f"âœ“ {csv_file.name}: {len(df)} registros carregados")
#             except UnicodeDecodeError:
#                 # Tenta com outra codificaÃ§Ã£o
#                 try:
#                     df = pd.read_csv(csv_file, encoding='latin-1')
#                     df['arquivo_origem'] = csv_file.name
#                     dataframes.append(df)
#                     st.success(f"âœ“ {csv_file.name}: {len(df)} registros carregados (latin-1)")
#                 except Exception as e:
#                     st.error(f"âœ— Erro ao carregar {csv_file.name}: {str(e)}")
#             except Exception as e:
#                 st.error(f"âœ— Erro ao carregar {csv_file.name}: {str(e)}")
        
#         if dataframes:
#             self.data = pd.concat(dataframes, ignore_index=True)
#             return True
#         return False
    
#     def load_data_from_upload(self, uploaded_files):
#         """Carrega dados dos arquivos CSV enviados via upload"""
#         dataframes = []
        
#         for uploaded_file in uploaded_files:
#             try:
#                 df = pd.read_csv(uploaded_file, encoding='utf-8')
#                 df['arquivo_origem'] = uploaded_file.name
#                 dataframes.append(df)
#                 st.success(f"âœ“ {uploaded_file.name}: {len(df)} registros carregados")
#             except Exception as e:
#                 st.error(f"âœ— Erro ao carregar {uploaded_file.name}: {str(e)}")
        
#         if dataframes:
#             self.data = pd.concat(dataframes, ignore_index=True)
#             return True
#         return False
    
#     def load_data_from_url(self, urls):
#         """Carrega dados de URLs (GitHub, Google Drive, etc)"""
#         dataframes = []
        
#         for url in urls:
#             try:
#                 # Ajusta URL do Google Drive se necessÃ¡rio
#                 if 'drive.google.com' in url:
#                     file_id = url.split('/d/')[1].split('/')[0]
#                     url = f'https://drive.google.com/uc?id={file_id}'
                
#                 df = pd.read_csv(url)
#                 df['arquivo_origem'] = f"URL_{len(dataframes)+1}"
#                 dataframes.append(df)
#                 st.success(f"âœ“ URL {len(dataframes)}: {len(df)} registros carregados")
#             except Exception as e:
#                 st.error(f"âœ— Erro ao carregar URL: {str(e)}")
        
#         if dataframes:
#             self.data = pd.concat(dataframes, ignore_index=True)
#             return True
#         return False
    
#     def preprocess_data(self):
#         """Preprocessa os dados"""
#         if self.data is None:
#             return False
        
#         # Limpeza de colunas
#         self.data.columns = self.data.columns.str.strip()
        
#         # ConversÃ£o de datas
#         if 'Data IncorporaÃ§Ã£o' in self.data.columns:
#             self.data['Data IncorporaÃ§Ã£o'] = pd.to_datetime(self.data['Data IncorporaÃ§Ã£o'], errors='coerce')
#             self.data['Ano IncorporaÃ§Ã£o'] = self.data['Data IncorporaÃ§Ã£o'].dt.year
        
#         # ConversÃ£o de similaridade
#         if 'Percentagem de Similaridade (%)' in self.data.columns:
#             self.data['Similaridade_Num'] = pd.to_numeric(
#                 self.data['Percentagem de Similaridade (%)'].str.replace('%', ''), 
#                 errors='coerce'
#             )
        
#         # ConversÃ£o de valores monetÃ¡rios
#         if 'Valor AquisiÃ§Ã£o' in self.data.columns:
#             self.data['Valor_Aquisicao_Num'] = pd.to_numeric(
#                 self.data['Valor AquisiÃ§Ã£o'].str.replace('R$', '').str.replace('.', '').str.replace(',', '.').str.strip(),
#                 errors='coerce'
#             )
        
#         if 'Valor ContÃ¡bil' in self.data.columns:
#             self.data['Valor_Contabil_Num'] = pd.to_numeric(
#                 self.data['Valor ContÃ¡bil'].str.replace('R$', '').str.replace('.', '').str.replace(',', '.').str.strip(),
#                 errors='coerce'
#             )
        
#         # ConversÃ£o de vida Ãºtil
#         if 'Vida' in self.data.columns:
#             self.data['Vida_Num'] = pd.to_numeric(self.data['Vida'], errors='coerce')
        
#         # CategorizaÃ§Ã£o
#         if 'Justificativa de Reprova' in self.data.columns:
#             self.data['Decisao'] = self.data['Justificativa de Reprova'].apply(self._categorizar_decisao)
#             self.data['Categoria_Similaridade'] = self.data['Similaridade_Num'].apply(self._categorizar_similaridade)
        
#         # LocalizaÃ§Ã£o
#         if 'LocalizaÃ§Ã£o' in self.data.columns:
#             self.data['Regiao'] = self.data['LocalizaÃ§Ã£o'].apply(self._extrair_regiao)
        
#         self.processed = True
#         return True
    
#     def _categorizar_decisao(self, justificativa):
#         if pd.isna(justificativa):
#             return "Sem Categoria"
        
#         justificativa = str(justificativa).upper()
        
#         if "RECLASSIFICAR" in justificativa:
#             return "RECLASSIFICAR"
#         elif "AVALIAR" in justificativa:
#             return "AVALIAR"
#         elif "MANTER" in justificativa:
#             return "MANTER"
#         else:
#             return "OUTROS"
    
#     def _categorizar_similaridade(self, similaridade):
#         if pd.isna(similaridade):
#             return "Sem Dados"
        
#         if similaridade >= 70:
#             return "Muito Alta (â‰¥70%)"
#         elif similaridade >= 50:
#             return "Alta (50-69%)"
#         elif similaridade >= 35:
#             return "Moderada (35-49%)"
#         else:
#             return "Baixa (<35%)"
    
#     def _extrair_regiao(self, localizacao):
#         if pd.isna(localizacao):
#             return "NÃ£o Informado"
        
#         return str(localizacao).strip()
    
#     def get_filtered_data(self, regiao_filter=None, decisao_filter=None, 
#                          ano_min=None, ano_max=None, similaridade_min=None):
#         """Retorna dados filtrados"""
#         if not self.processed:
#             return pd.DataFrame()
        
#         filtered = self.data.copy()
        
#         if regiao_filter and regiao_filter != "Todas":
#             filtered = filtered[filtered['Regiao'] == regiao_filter]
        
#         if decisao_filter and decisao_filter != "Todas":
#             filtered = filtered[filtered['Decisao'] == decisao_filter]
        
#         if ano_min is not None:
#             filtered = filtered[filtered['Ano IncorporaÃ§Ã£o'] >= ano_min]
        
#         if ano_max is not None:
#             filtered = filtered[filtered['Ano IncorporaÃ§Ã£o'] <= ano_max]
        
#         if similaridade_min is not None:
#             filtered = filtered[filtered['Similaridade_Num'] >= similaridade_min]
        
#         return filtered

# # FunÃ§Ãµes de visualizaÃ§Ã£o
# def create_decision_pie_chart(data):
#     """GrÃ¡fico de pizza das decisÃµes"""
#     decisoes = data['Decisao'].value_counts()
    
#     fig = px.pie(
#         values=decisoes.values, 
#         names=decisoes.index,
#         title="DistribuiÃ§Ã£o de DecisÃµes",
#         color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
#     )
    
#     fig.update_traces(textposition='inside', textinfo='percent+label')
#     return fig

# def create_similarity_histogram(data):
#     """Histograma de similaridade"""
#     fig = px.histogram(
#         data, 
#         x='Similaridade_Num',
#         nbins=20,
#         title="DistribuiÃ§Ã£o de Similaridade (%)",
#         labels={'Similaridade_Num': 'Similaridade (%)', 'count': 'Quantidade'}
#     )
    
#     fig.update_layout(showlegend=False)
#     return fig

# def create_regional_bar_chart(data):
#     """GrÃ¡fico de barras por localizaÃ§Ã£o"""
#     localizacao_counts = data['Regiao'].value_counts()
    
#     fig = px.bar(
#         x=localizacao_counts.index,
#         y=localizacao_counts.values,
#         title="Itens por LocalizaÃ§Ã£o",
#         labels={'x': 'LocalizaÃ§Ã£o', 'y': 'Quantidade'}
#     )
    
#     fig.update_layout(
#         showlegend=False,
#         xaxis=dict(tickangle=45)
#     )
#     return fig

# def create_similarity_box_plot(data):
#     """Box plot de similaridade por decisÃ£o"""
#     fig = px.box(
#         data,
#         x='Decisao',
#         y='Similaridade_Num',
#         title="Similaridade por Tipo de DecisÃ£o",
#         labels={'Similaridade_Num': 'Similaridade (%)', 'Decisao': 'DecisÃ£o'}
#     )
    
#     return fig

# def create_timeline_chart(data):
#     """GrÃ¡fico temporal"""
#     timeline = data.groupby(['Ano IncorporaÃ§Ã£o', 'Decisao']).size().unstack(fill_value=0)
    
#     fig = go.Figure()
    
#     colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
#     for i, decisao in enumerate(timeline.columns):
#         fig.add_trace(
#             go.Scatter(
#                 x=timeline.index,
#                 y=timeline[decisao],
#                 mode='lines+markers',
#                 name=decisao,
#                 line=dict(color=colors[i % len(colors)])
#             )
#         )
    
#     fig.update_layout(
#         title="Timeline de IncorporaÃ§Ãµes por DecisÃ£o",
#         xaxis_title="Ano",
#         yaxis_title="Quantidade",
#         hovermode='x unified'
#     )
    
#     return fig

# def create_strategic_metrics(data):
#     """MÃ©tricas estratÃ©gicas"""
#     total_itens = len(data)
    
#     if total_itens == 0:
#         return {}, {}
    
#     # Contadores por decisÃ£o
#     decisoes = data['Decisao'].value_counts()
#     reclassificar = decisoes.get('RECLASSIFICAR', 0)
#     avaliar = decisoes.get('AVALIAR', 0)
#     manter = decisoes.get('MANTER', 0)
    
#     # EstatÃ­sticas de similaridade
#     similaridade_stats = data['Similaridade_Num'].describe()
    
#     # MÃ©tricas principais
#     metrics = {
#         'total_itens': total_itens,
#         'reclassificar': reclassificar,
#         'avaliar': avaliar,
#         'manter': manter,
#         'percentual_reclassificar': (reclassificar / total_itens) * 100,
#         'percentual_avaliar': (avaliar / total_itens) * 100,
#         'similaridade_media': similaridade_stats.get('mean', 0),
#         'alta_similaridade': len(data[data['Similaridade_Num'] >= 70])
#     }
    
#     # AnÃ¡lise regional
#     regional_analysis = data.groupby('Regiao').agg({
#         'Decisao': lambda x: (x == 'RECLASSIFICAR').sum(),
#         'Similaridade_Num': 'mean'
#     }).round(2)
    
#     regional_analysis.columns = ['Itens_Reclassificar', 'Similaridade_Media']
    
#     return metrics, regional_analysis

# def export_to_excel(data):
#     """Exporta dados para Excel"""
#     output = BytesIO()
    
#     with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
#         # Dados principais
#         data.to_excel(writer, sheet_name='Dados_Completos', index=False)
        
#         # Resumo por decisÃ£o
#         decisao_summary = data.groupby('Decisao').agg({
#             'Similaridade_Num': ['count', 'mean', 'min', 'max'],
#             'Ano IncorporaÃ§Ã£o': ['min', 'max']
#         }).round(2)
#         decisao_summary.to_excel(writer, sheet_name='Resumo_Decisoes')
        
#         # Resumo regional
#         regional_summary = data.groupby('Regiao').agg({
#             'Decisao': lambda x: x.value_counts().to_dict(),
#             'Similaridade_Num': ['count', 'mean']
#         })
#         regional_summary.to_excel(writer, sheet_name='Resumo_Regional')
    
#     return output.getvalue()

# def create_demo_data():
#     """Cria dados de demonstraÃ§Ã£o"""
#     np.random.seed(42)
#     n_samples = 1000
    
#     sample_data = {
#         'LocalizaÃ§Ã£o': np.random.choice(['1.01 BRÃS', '1.02 VILA ALPINA', '2.01 SANTOS', 'SENAI SEDE', '3.01 TAUBATÃ‰'], n_samples),
#         'InventÃ¡rio': range(1000000, 1000000 + n_samples),
#         'DenominaÃ§Ã£o do Imobilizado': [f"EQUIPAMENTO TESTE {i}" for i in range(n_samples)],
#         'Vida': np.random.choice([10, 15, 20, 25, 50], n_samples),
#         'Valor AquisiÃ§Ã£o': [f"R$ {np.random.uniform(100, 50000):.2f}".replace('.', ',') for _ in range(n_samples)],
#         'Valor ContÃ¡bil': [f"R$ {np.random.uniform(0, 30000):.2f}".replace('.', ',') for _ in range(n_samples)],
#         'Item ConsumÃ­vel Similar': [f"CONSUMÃVEL SIMILAR {i}" for i in range(n_samples)],
#         'Percentagem de Similaridade (%)': [f"{np.random.uniform(20, 85):.1f}%" for _ in range(n_samples)],
#         'Data IncorporaÃ§Ã£o': pd.date_range('2010-01-01', '2025-01-01', periods=n_samples),
#         'Justificativa de Reprova': np.random.choice([
#             'RECLASSIFICAR: Alta similaridade (70%+)',
#             'AVALIAR: Similaridade moderada (45%)',
#             'MANTER: Similaridade baixa (25%)'
#         ], n_samples, p=[0.25, 0.45, 0.3])
#     }
    
#     return pd.DataFrame(sample_data)

# # Interface Streamlit
# def main():
#     st.title("ğŸ“Š Sistema de AnÃ¡lise de PatrimÃ´nio")
#     st.markdown("---")
    
#     # Inicializar analyzer na sessÃ£o
#     if 'analyzer' not in st.session_state:
#         st.session_state.analyzer = PatrimonioAnalyzer()
    
#     analyzer = st.session_state.analyzer
    
#     # Sidebar para configuraÃ§Ãµes de carregamento
#     with st.sidebar:
#         st.header("âš™ï¸ ConfiguraÃ§Ã£o de Dados")
        
#         # Modo de carregamento
#         load_mode = st.radio(
#             "Modo de Carregamento:",
#             ["ğŸ—‚ï¸ Pasta Local (Auto)", "ğŸ“¤ Upload Manual", "ğŸ”— URLs Remotas", "ğŸ¯ DemonstraÃ§Ã£o"],
#             help="Escolha como carregar os dados"
#         )
        
#         st.markdown("---")
        
#         # Carregamento baseado no modo selecionado
#         if load_mode == "ğŸ—‚ï¸ Pasta Local (Auto)":
#             st.info("ğŸ“‚ Carregamento automÃ¡tico da pasta 'data/'")
            
#             folder_path = st.text_input("Caminho da pasta:", value="data")
            
#             if st.button("ğŸ”„ Carregar Dados"):
#                 with st.spinner("Carregando arquivos..."):
#                     if analyzer.load_data_from_folder(folder_path):
#                         if analyzer.preprocess_data():
#                             st.success(f"âœ“ {len(analyzer.data)} registros carregados!")
#                         else:
#                             st.error("Erro no processamento dos dados")
            
#             # Auto-load na inicializaÃ§Ã£o
#             if not analyzer.processed and 'auto_loaded' not in st.session_state:
#                 with st.spinner("Carregando dados automaticamente..."):
#                     if analyzer.load_data_from_folder(folder_path):
#                         if analyzer.preprocess_data():
#                             st.success(f"âœ“ {len(analyzer.data)} registros carregados automaticamente!")
#                             st.session_state.auto_loaded = True
        
#         elif load_mode == "ğŸ“¤ Upload Manual":
#             uploaded_files = st.file_uploader(
#                 "Selecione os arquivos CSV",
#                 type=['csv'],
#                 accept_multiple_files=True,
#                 help="VocÃª pode selecionar atÃ© 12 arquivos CSV"
#             )
            
#             if uploaded_files:
#                 if st.button("ğŸ“¥ Carregar Arquivos"):
#                     with st.spinner("Carregando arquivos..."):
#                         if analyzer.load_data_from_upload(uploaded_files):
#                             if analyzer.preprocess_data():
#                                 st.success(f"âœ“ {len(analyzer.data)} registros carregados!")
#                             else:
#                                 st.error("Erro no processamento dos dados")
        
#         elif load_mode == "ğŸ”— URLs Remotas":
#             st.info("ğŸ’¡ Suporta GitHub Raw, Google Drive pÃºblico, etc.")
            
#             url_input = st.text_area(
#                 "URLs (uma por linha):",
#                 help="Cole as URLs dos arquivos CSV, uma por linha"
#             )
            
#             if st.button("ğŸŒ Carregar de URLs"):
#                 urls = [url.strip() for url in url_input.split('\n') if url.strip()]
#                 if urls:
#                     with st.spinner("Baixando arquivos..."):
#                         if analyzer.load_data_from_url(urls):
#                             if analyzer.preprocess_data():
#                                 st.success(f"âœ“ {len(analyzer.data)} registros carregados!")
#                             else:
#                                 st.error("Erro no processamento dos dados")
        
#         else:  # DemonstraÃ§Ã£o
#             if st.button("ğŸ¯ Gerar Dados de DemonstraÃ§Ã£o"):
#                 with st.spinner("Criando dados de demonstraÃ§Ã£o..."):
#                     analyzer.data = create_demo_data()
#                     analyzer.preprocess_data()
#                     st.success("âœ“ Dados de demonstraÃ§Ã£o carregados!")
    
#     # Verificar se dados foram carregados
#     if not analyzer.processed:
#         st.info("ğŸ‘† Configure o modo de carregamento na barra lateral para comeÃ§ar")
        
#         # InstruÃ§Ãµes
#         with st.expander("ğŸ“– Como usar cada modo"):
#             st.markdown("""
#             ### ğŸ—‚ï¸ Pasta Local (Auto)
#             - **Para Streamlit Cloud**: Crie uma pasta `data/` no seu repositÃ³rio
#             - Coloque seus arquivos CSV dentro dessa pasta
#             - Os dados serÃ£o carregados automaticamente no deploy
            
#             ### ğŸ“¤ Upload Manual
#             - Use para testes locais ou dados temporÃ¡rios
#             - Arraste e solte seus CSVs
            
#             ### ğŸ”— URLs Remotas
#             - **GitHub**: Use URL raw (ex: `https://raw.githubusercontent.com/user/repo/main/data.csv`)
#             - **Google Drive**: Compartilhe o arquivo publicamente e use o link
#             - **Outros**: Qualquer URL pÃºblica de CSV
            
#             ### ğŸ¯ DemonstraÃ§Ã£o
#             - Gera 1000 registros de exemplo para testar a aplicaÃ§Ã£o
#             """)
        
#         return
    
#     # Filtros na sidebar
#     with st.sidebar:
#         st.markdown("---")
#         st.header("ğŸ”§ Filtros")
        
#         # Filtro de localizaÃ§Ã£o
#         localizacoes = ["Todas"] + sorted(analyzer.data['Regiao'].unique().tolist())
#         regiao_filter = st.selectbox("LocalizaÃ§Ã£o", localizacoes)
        
#         # Filtro de decisÃ£o
#         decisoes = ["Todas"] + sorted(analyzer.data['Decisao'].unique().tolist())
#         decisao_filter = st.selectbox("DecisÃ£o", decisoes)
        
#         # Filtro de ano
#         anos = analyzer.data['Ano IncorporaÃ§Ã£o'].dropna()
#         if len(anos) > 0:
#             ano_min, ano_max = st.slider(
#                 "PerÃ­odo de IncorporaÃ§Ã£o",
#                 int(anos.min()),
#                 int(anos.max()),
#                 (int(anos.min()), int(anos.max()))
#             )
#         else:
#             ano_min, ano_max = None, None
        
#         # Filtro de similaridade mÃ­nima
#         similaridade_min = st.slider(
#             "Similaridade MÃ­nima (%)",
#             0, 100, 0
#         )
        
#         # Aplicar filtros
#         filtered_data = analyzer.get_filtered_data(
#             regiao_filter, decisao_filter, ano_min, ano_max, similaridade_min
#         )
    
#     # MÃ©tricas principais
#     st.header("ğŸ“ˆ Resumo Executivo")
    
#     metrics, regional_analysis = create_strategic_metrics(filtered_data)
    
#     if metrics:
#         col1, col2, col3, col4 = st.columns(4)
        
#         with col1:
#             st.metric(
#                 "Total de Itens",
#                 f"{metrics['total_itens']:,}",
#                 help="NÃºmero total de itens na anÃ¡lise"
#             )
        
#         with col2:
#             st.metric(
#                 "Para Reclassificar",
#                 f"{metrics['reclassificar']:,}",
#                 f"{metrics['percentual_reclassificar']:.1f}%",
#                 help="Itens recomendados para reclassificaÃ§Ã£o"
#             )
        
#         with col3:
#             st.metric(
#                 "Para Avaliar",
#                 f"{metrics['avaliar']:,}",
#                 f"{metrics['percentual_avaliar']:.1f}%",
#                 help="Itens que necessitam avaliaÃ§Ã£o adicional"
#             )
        
#         with col4:
#             st.metric(
#                 "Similaridade MÃ©dia",
#                 f"{metrics['similaridade_media']:.1f}%",
#                 help="Percentual mÃ©dio de similaridade com consumÃ­veis"
#             )
    
#     # Abas para diferentes anÃ¡lises
#     tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š VisÃ£o Geral", "ğŸ” AnÃ¡lise Detalhada", "ğŸ¢ AnÃ¡lise por LocalizaÃ§Ã£o", "ğŸ“‹ Dados e Export"])
    
#     with tab1:
#         st.subheader("VisÃ£o Geral dos Dados")
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             fig_pie = create_decision_pie_chart(filtered_data)
#             st.plotly_chart(fig_pie, use_container_width=True)
            
#             fig_hist = create_similarity_histogram(filtered_data)
#             st.plotly_chart(fig_hist, use_container_width=True)
        
#         with col2:
#             fig_bar = create_regional_bar_chart(filtered_data)
#             st.plotly_chart(fig_bar, use_container_width=True)
            
#             fig_box = create_similarity_box_plot(filtered_data)
#             st.plotly_chart(fig_box, use_container_width=True)
    
#     with tab2:
#         st.subheader("AnÃ¡lise Detalhada")
        
#         fig_timeline = create_timeline_chart(filtered_data)
#         st.plotly_chart(fig_timeline, use_container_width=True)
        
#         st.subheader("âš ï¸ Itens de Alta Prioridade")
        
#         alta_similaridade = filtered_data[filtered_data['Similaridade_Num'] >= 70]
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             st.info(f"**{len(alta_similaridade):,} itens** com similaridade â‰¥ 70%")
            
#             if len(alta_similaridade) > 0:
#                 top_regioes = alta_similaridade['Regiao'].value_counts().head()
#                 st.write("**LocalizaÃ§Ãµes com mais itens de alta similaridade:**")
#                 for localizacao, count in top_regioes.items():
#                     st.write(f"â€¢ {localizacao}: {count} itens")
        
#         with col2:
#             reclassificacao_imediata = filtered_data[filtered_data['Decisao'] == 'RECLASSIFICAR']
#             st.warning(f"**{len(reclassificacao_imediata):,} itens** para reclassificaÃ§Ã£o imediata")
            
#             if len(reclassificacao_imediata) > 0:
#                 similaridade_media = reclassificacao_imediata['Similaridade_Num'].mean()
#                 st.write(f"**Similaridade mÃ©dia:** {similaridade_media:.1f}%")
    
#     with tab3:
#         st.subheader("AnÃ¡lise por LocalizaÃ§Ã£o")
        
#         if not regional_analysis.empty:
#             st.write("**Resumo por LocalizaÃ§Ã£o:**")
            
#             regional_display = regional_analysis.copy()
#             regional_display['EficiÃªncia_ReclassificaÃ§Ã£o'] = (
#                 regional_display['Itens_Reclassificar'] / 
#                 filtered_data.groupby('Regiao').size()
#             ) * 100
            
#             regional_display = regional_display.round(2)
#             regional_display.columns = ['Itens p/ Reclassificar', 'Similaridade MÃ©dia (%)', 'EficiÃªncia (%)']
            
#             st.dataframe(regional_display.sort_values('EficiÃªncia (%)', ascending=False))
            
#             fig = px.scatter(
#                 x=regional_display['Similaridade MÃ©dia (%)'],
#                 y=regional_display['EficiÃªncia (%)'],
#                 size=regional_display['Itens p/ Reclassificar'],
#                 hover_name=regional_display.index,
#                 title="EficiÃªncia por LocalizaÃ§Ã£o: Similaridade vs % ReclassificaÃ§Ã£o",
#                 labels={
#                     'x': 'Similaridade MÃ©dia (%)',
#                     'y': 'EficiÃªncia na ReclassificaÃ§Ã£o (%)'
#                 }
#             )
            
#             st.plotly_chart(fig, use_container_width=True)
    
#     with tab4:
#         st.subheader("Dados e ExportaÃ§Ã£o")
        
#         st.write(f"**Mostrando {len(filtered_data):,} registros filtrados:**")
        
#         show_columns = st.multiselect(
#             "Selecione as colunas para exibir:",
#             filtered_data.columns.tolist(),
#             default=['LocalizaÃ§Ã£o', 'InventÃ¡rio', 'DenominaÃ§Ã£o do Imobilizado', 'Item ConsumÃ­vel Similar', 'Decisao']
#         )
        
#         if show_columns:
#             st.dataframe(filtered_data[show_columns])
        
#         st.subheader("ğŸ’¾ Exportar Dados")
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             csv = filtered_data.to_csv(index=False)
#             st.download_button(
#                 label="ğŸ“„ Baixar CSV",
#                 data=csv,
#                 file_name=f"patrimonio_filtrado_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
#                 mime="text/csv"
#             )
        
#         with col2:
#             excel_data = export_to_excel(filtered_data)
#             st.download_button(
#                 label="ğŸ“Š Baixar Excel",
#                 data=excel_data,
#                 file_name=f"patrimonio_analise_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
#                 mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#             )

# if __name__ == "__main__":
#     main()


